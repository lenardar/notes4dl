{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983ab50a",
   "metadata": {},
   "source": [
    "# 情感分析及数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2bb6115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59aef7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在本地找到已缓存的数据目录: ./data/aclImdb\n",
      "\n",
      "成功获取 aclImdb 数据目录路径: ./data/aclImdb\n",
      "在 ./data/aclImdb/train/pos 中找到训练数据，解压成功！\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import hashlib\n",
    "import tarfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_imdb(cache_dir=os.path.join('.', 'data')):\n",
    "    \"\"\"\n",
    "    一个独立的函数，用于下载、校验和解压aclImdb数据集。\n",
    "    \"\"\"\n",
    "    # 1. 定义数据源信息 (URL 和 SHA-1 校验和)\n",
    "    url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "    sha1_hash = '01ada507287d82875905620988597833ad4e0903'\n",
    "    \n",
    "    # 2. 创建缓存目录和文件路径\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    archive_path = os.path.join(cache_dir, 'aclImdb_v1.tar.gz')\n",
    "    # 解压后会生成一个名为 'aclImdb' 的文件夹\n",
    "    dest_dir = os.path.join(cache_dir, 'aclImdb')\n",
    "\n",
    "    # 3. 检查本地是否有最终解压好的文件夹，如果没有则下载\n",
    "    if not os.path.exists(dest_dir):\n",
    "        print(f\"本地未找到 '{dest_dir}'，开始下载流程...\")\n",
    "        # 下载 .tar.gz 文件\n",
    "        print(f\"正在从 {url} 下载...\")\n",
    "        try:\n",
    "            r = requests.get(url, stream=True, timeout=120) # 考虑到文件较大，增加超时\n",
    "            r.raise_for_status()\n",
    "            total_size = int(r.headers.get('content-length', 0))\n",
    "            with open(archive_path, 'wb') as f, tqdm(\n",
    "                desc='aclImdb_v1.tar.gz', total=total_size, unit='iB', unit_scale=True\n",
    "            ) as bar:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "                    bar.update(len(chunk))\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if os.path.exists(archive_path): os.remove(archive_path)\n",
    "            raise IOError(f\"下载文件时出错: {e}\")\n",
    "\n",
    "        # 4. 校验文件完整性\n",
    "        print(\"正在校验文件...\")\n",
    "        sha1 = hashlib.sha1()\n",
    "        with open(archive_path, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(1048576) # 1MB 块\n",
    "                if not data: break\n",
    "                sha1.update(data)\n",
    "        if sha1.hexdigest() != sha1_hash:\n",
    "            raise IOError(f\"文件 {archive_path} SHA1 校验和不匹配！\")\n",
    "        \n",
    "        # 5. 解压 .tar.gz 文件\n",
    "        print(f\"正在解压 {archive_path}...\")\n",
    "        with tarfile.open(archive_path, 'r:gz') as tf:\n",
    "            tf.extractall(cache_dir)\n",
    "        os.remove(archive_path) # 删除 .tar.gz 文件节省空间\n",
    "        print(\"数据准备完成。\")\n",
    "    else:\n",
    "        print(f\"在本地找到已缓存的数据目录: {dest_dir}\")\n",
    "\n",
    "    return dest_dir\n",
    "\n",
    "# --- 如何使用 ---\n",
    "# 只有当此脚本作为主程序运行时，以下代码块才会执行\n",
    "if __name__ == '__main__':\n",
    "    # 你需要安装requests和tqdm: pip install requests tqdm\n",
    "    \n",
    "    # 调用函数获取数据目录\n",
    "    # 首次运行时会自动下载 (约80MB)，之后会直接使用本地缓存\n",
    "    try:\n",
    "        data_dir = download_imdb()\n",
    "        \n",
    "        # 打印路径以验证\n",
    "        print(f\"\\n成功获取 aclImdb 数据目录路径: {data_dir}\")\n",
    "        \n",
    "        # 我们可以检查一下目录内容来确认解压是否成功\n",
    "        # 例如，检查训练数据中的正面评论文件夹是否存在\n",
    "        train_pos_path = os.path.join(data_dir, 'train', 'pos')\n",
    "        if os.path.exists(train_pos_path):\n",
    "            print(f\"在 {train_pos_path} 中找到训练数据，解压成功！\")\n",
    "        else:\n",
    "            print(\"错误：未找到预期的训练数据文件夹。\")\n",
    "            \n",
    "    except (IOError, requests.exceptions.RequestException) as e:\n",
    "        print(f\"\\n处理失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fcd0ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数目： 25000\n",
      "标签： 1 review: For a movie that gets no respect there sure are a lot of mem\n",
      "标签： 1 review: Bizarre horror movie filled with famous faces but stolen by \n",
      "标签： 1 review: A solid, if unremarkable film. Matthau, as Einstein, was won\n"
     ]
    }
   ],
   "source": [
    "#@save\n",
    "def read_imdb(data_dir, is_train):\n",
    "    \"\"\"读取IMDb评论数据集文本序列和标签\"\"\"\n",
    "    data, labels = [], []\n",
    "    for label in ('pos', 'neg'):\n",
    "        folder_name = os.path.join(data_dir, 'train' if is_train else 'test',\n",
    "                                   label)\n",
    "        for file in os.listdir(folder_name):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '')\n",
    "                data.append(review)\n",
    "                labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "train_data = read_imdb(data_dir, is_train=True)\n",
    "print('训练集数目：', len(train_data[0]))\n",
    "for x, y in zip(train_data[0][:3], train_data[1][:3]):\n",
    "    print('标签：', y, 'review:', x[0:60])\n",
    "\n",
    "# 训练集数目： 25000\n",
    "# 标签： 1 review: For a movie that gets no respect there sure are a lot of mem\n",
    "# 标签： 1 review: Bizarre horror movie filled with famous faces but stolen by \n",
    "# 标签： 1 review: A solid, if unremarkable film. Matthau, as Einstein, was won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bdce734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Vocab:  #@save\n",
    "    \"\"\"文本词表\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # 按出现频率排序\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # 未知词元的索引为0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # 未知词元的索引为0\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "\n",
    "def count_corpus(tokens):  #@save\n",
    "    \"\"\"统计词元的频率\"\"\"\n",
    "    # 这里的tokens是1D列表或2D列表\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # 将词元列表展平成一个列表\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c42494ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lines, token='word'):  #@save\n",
    "    \"\"\"将文本行拆分为单词或字符词元\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('错误：未知词元类型：' + token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0fe315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = tokenize(train_data[0], token='word')\n",
    "vocab = Vocab(train_tokens, min_freq=5, reserved_tokens=['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e0b02e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25000, 500])\n"
     ]
    }
   ],
   "source": [
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    \"\"\"截断或填充文本序列\"\"\"\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]  # 截断\n",
    "    return line + [padding_token] * (num_steps - len(line))  # 填充\n",
    "\n",
    "num_steps = 500  # 序列长度\n",
    "train_features = torch.tensor([truncate_pad(\n",
    "    vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83d47923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([64, 500]) , y: torch.Size([64])\n",
      "小批量数目： 391\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"\n",
    "    将内存中的数据（如NumPy数组、列表或PyTorch张量）封装成PyTorch数据迭代器。\n",
    "    这是一个 d2l.load_array 函数的独立实现。\n",
    "\n",
    "    参数:\n",
    "        data_arrays (tuple or list): 包含特征、标签等数据数组的元组或列表。\n",
    "                                     所有数组的第一个维度（样本数）必须相同。\n",
    "        batch_size (int): 每个小批量的大小。\n",
    "        is_train (bool): 如果为True，则在每个周期打乱数据顺序；否则不打乱。\n",
    "\n",
    "    返回:\n",
    "        一个PyTorch数据迭代器 (torch.utils.data.DataLoader)。\n",
    "    \"\"\"\n",
    "    # 1. 将所有输入数组转换为 PyTorch 的 Tensor\n",
    "    #    这里假设输入可以是 list, numpy.ndarray, 或 torch.Tensor\n",
    "    dataset_tensors = [torch.tensor(data) if not isinstance(data, torch.Tensor) else data \n",
    "                       for data in data_arrays]\n",
    "    \n",
    "    # 2. 使用 TensorDataset 将数据打包\n",
    "    #    *dataset_tensors 会将列表中的每个张量解包作为独立的参数传入\n",
    "    dataset = TensorDataset(*dataset_tensors)\n",
    "    \n",
    "    # 3. 使用 DataLoader 创建最终的数据迭代器\n",
    "    #    shuffle 参数根据 is_train 的值来决定\n",
    "    data_iter = DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "    \n",
    "    return data_iter\n",
    "\n",
    "train_iter = load_array((train_features,\n",
    "    torch.tensor(train_data[1])), 64)\n",
    "\n",
    "for X, y in train_iter:\n",
    "    print('X:', X.shape, ', y:', y.shape)\n",
    "    break\n",
    "print('小批量数目：', len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2026e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def load_data_imdb(batch_size, num_steps=500):\n",
    "    \"\"\"返回数据迭代器和IMDb评论数据集的词表\"\"\"\n",
    "    data_dir = download_imdb()\n",
    "    train_data = read_imdb(data_dir, True)\n",
    "    test_data = read_imdb(data_dir, False)\n",
    "    train_tokens = tokenize(train_data[0], token='word')\n",
    "    test_tokens = tokenize(test_data[0], token='word')\n",
    "    vocab = Vocab(train_tokens, min_freq=5)\n",
    "    train_features = torch.tensor([truncate_pad(\n",
    "        vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\n",
    "    test_features = torch.tensor([truncate_pad(\n",
    "        vocab[line], num_steps, vocab['<pad>']) for line in test_tokens])\n",
    "    train_iter = load_array((train_features, torch.tensor(train_data[1])),\n",
    "                                batch_size)\n",
    "    test_iter = load_array((test_features, torch.tensor(test_data[1])),\n",
    "                               batch_size,\n",
    "                               is_train=False)\n",
    "    return train_iter, test_iter, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f5065b",
   "metadata": {},
   "source": [
    "# 情感分析：使用循环神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26678ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在本地找到已缓存的数据目录: ./data/aclImdb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "batch_size = 64\n",
    "train_iter, test_iter, vocab = load_data_imdb(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82f4e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens,\n",
    "                 num_layers, **kwargs):\n",
    "        super(BiRNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # 将bidirectional设置为True以获取双向循环神经网络\n",
    "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers,\n",
    "                                bidirectional=True)\n",
    "        self.decoder = nn.Linear(4 * num_hiddens, 2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs的形状是（批量大小，时间步数）\n",
    "        # 因为长短期记忆网络要求其输入的第一个维度是时间维，\n",
    "        # 所以在获得词元表示之前，输入会被转置。\n",
    "        # 输出形状为（时间步数，批量大小，词向量维度）\n",
    "        embeddings = self.embedding(inputs.T)\n",
    "        self.encoder.flatten_parameters()\n",
    "        # 返回上一个隐藏层在不同时间步的隐状态，\n",
    "        # outputs的形状是（时间步数，批量大小，2*隐藏单元数）\n",
    "        outputs, _ = self.encoder(embeddings)\n",
    "        # 连结初始和最终时间步的隐状态，作为全连接层的输入，\n",
    "        # 其形状为（批量大小，4*隐藏单元数）\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), dim=1)\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "614af1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers = 100, 100, 2\n",
    "devices = torch.device('mps' if torch.mps.is_available() else 'cpu')\n",
    "net = BiRNN(len(vocab), embed_size, num_hiddens, num_layers)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    if type(m) == nn.LSTM:\n",
    "        for param in m._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(m._parameters[param])\n",
    "net.apply(init_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88094ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _download_embedding_if_needed(name, cache_dir=os.path.join('.', 'data')):\n",
    "    \"\"\"\n",
    "    一个独立的辅助函数，用于下载和解压GloVe词向量。\n",
    "    该版本能自动在压缩包内查找正确的 .txt 文件名，并支持缓存。\n",
    "    \"\"\"\n",
    "    # 数据源信息库：包含URL\n",
    "    DATA_HUB = {\n",
    "        'glove.6b.50d': ('http://d2l-data.s3-accelerate.amazonaws.com/glove.6B.50d.zip',),\n",
    "        'glove.6b.100d': ('http://d2l-data.s3-accelerate.amazonaws.com/glove.6B.100d.zip',)\n",
    "    }\n",
    "    \n",
    "    if name not in DATA_HUB:\n",
    "        raise ValueError(f\"未定义的数据集名称: {name}\")\n",
    "\n",
    "    url, = DATA_HUB[name]\n",
    "    \n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    zip_path = os.path.join(cache_dir, url.split('/')[-1])\n",
    "    \n",
    "    # 动态确定解压后的文件名，以增加代码的健壮性\n",
    "    # 通常文件名与下载名类似，例如 'glove.6b.100d.txt'\n",
    "    # 我们先检查一个可能的文件名，如果不存在再执行下载解压\n",
    "    potential_fname = name + '.txt'\n",
    "    embedding_path = os.path.join(cache_dir, potential_fname)\n",
    "    \n",
    "    # 关键的缓存检查：如果最终文件不存在，才执行下载解压\n",
    "    if not os.path.exists(embedding_path):\n",
    "        print(f\"本地未找到词向量文件，开始下载和解压流程...\")\n",
    "        \n",
    "        # 1. 下载 .zip 文件 (如果压缩包也不存在)\n",
    "        if not os.path.exists(zip_path):\n",
    "            print(f\"正在下载 {url} ...\")\n",
    "            try:\n",
    "                r = requests.get(url, stream=True, timeout=60)\n",
    "                r.raise_for_status()\n",
    "                total_size = int(r.headers.get('content-length', 0))\n",
    "                with open(zip_path, 'wb') as f, tqdm(\n",
    "                    desc=name, total=total_size, unit='iB', unit_scale=True\n",
    "                ) as bar:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                        bar.update(len(chunk))\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if os.path.exists(zip_path): os.remove(zip_path)\n",
    "                raise IOError(f\"下载文件时出错: {e}\")\n",
    "\n",
    "        # 2. 解压 .zip 文件\n",
    "        print(f\"正在解压 {zip_path}...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "            # 自动在压缩包内查找 .txt 文件\n",
    "            txt_filename = ''\n",
    "            for file_in_zip in zf.namelist():\n",
    "                if file_in_zip.lower().endswith('.txt'):\n",
    "                    txt_filename = file_in_zip\n",
    "                    break\n",
    "            \n",
    "            if not txt_filename:\n",
    "                raise IOError(f\"在 {zip_path} 中未找到 .txt 文件。\")\n",
    "            \n",
    "            print(f\"在压缩包中找到文件: {txt_filename}, 正在解压...\")\n",
    "            zf.extract(txt_filename, cache_dir)\n",
    "            # 更新为正确的最终文件路径\n",
    "            embedding_path = os.path.join(cache_dir, txt_filename)\n",
    "        \n",
    "        os.remove(zip_path) # 操作完成后删除zip文件以节省空间\n",
    "        print(\"下载和解压完成。\")\n",
    "    else:\n",
    "        print(f\"在本地找到已缓存的文件: {embedding_path}\")\n",
    "\n",
    "    return embedding_path\n",
    "\n",
    "\n",
    "class TokenEmbedding:\n",
    "    \"\"\"\n",
    "    一个独立的GloVe嵌入加载类，功能与d2l.TokenEmbedding类似。\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_name):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding(embedding_name)\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {token: idx for idx, token in\n",
    "                             enumerate(self.idx_to_token)}\n",
    "        print(f\"成功加载 '{embedding_name}'。词汇表大小: {len(self.idx_to_token)}\")\n",
    "\n",
    "    def _load_embedding(self, embedding_name):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "        embedding_path = _download_embedding_if_needed(embedding_name)\n",
    "        \n",
    "        with open(embedding_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "        \n",
    "        embedding_dim = len(idx_to_vec[0])\n",
    "        idx_to_vec = [[0.0] * embedding_dim] + idx_to_vec\n",
    "        return idx_to_token, torch.tensor(idx_to_vec, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            tokens = [tokens]\n",
    "        indices = [self.token_to_idx.get(token, self.unknown_idx) for token in tokens]\n",
    "        vecs = self.idx_to_vec[torch.tensor(indices)]\n",
    "        return vecs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "870cfd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本地未找到词向量文件，开始下载和解压流程...\n",
      "正在下载 http://d2l-data.s3-accelerate.amazonaws.com/glove.6B.100d.zip ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "glove.6b.100d: 100%|██████████| 134M/134M [00:12<00:00, 10.9MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在解压 ./data/glove.6B.100d.zip...\n",
      "在压缩包中找到文件: glove.6B.100d/vec.txt, 正在解压...\n",
      "下载和解压完成。\n",
      "成功加载 'glove.6b.100d'。词汇表大小: 400001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([49346, 100])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding = TokenEmbedding('glove.6b.100d')\n",
    "embeds = glove_embedding[vocab.idx_to_token]\n",
    "embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8b9ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.embedding.weight.data.copy_(embeds)\n",
    "net.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "69f5bc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 mps 上开始训练...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 88\u001b[39m\n\u001b[32m     86\u001b[39m loss = nn.CrossEntropyLoss(reduction=\u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     87\u001b[39m devices = torch.device(\u001b[33m'\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.mps.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[43mtrain_ch13\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mtrain_ch13\u001b[39m\u001b[34m(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\u001b[39m\n\u001b[32m     64\u001b[39m     num_samples += y.numel()\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# 评估过程\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m test_acc = \u001b[43mevaluate_accuracy_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# 打印信息\u001b[39;00m\n\u001b[32m     70\u001b[39m epoch_time = time.time() - epoch_start_time\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mevaluate_accuracy_gpu\u001b[39m\u001b[34m(net, data_iter, device)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     27\u001b[39m     X = X.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m y = \u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m metric[\u001b[32m0\u001b[39m] += accuracy(net(X), y)\n\u001b[32m     30\u001b[39m metric[\u001b[32m1\u001b[39m] += y.numel()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "import random\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \"\"\"计算预测正确的数量\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    \"\"\"使用GPU计算模型在数据集上的精度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()  # 设置为评估模式\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    \n",
    "    metric = [0.0, 0]  # 正确预测数，总预测数\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric[0] += accuracy(net(X), y)\n",
    "            metric[1] += y.numel()\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices):\n",
    "    \"\"\"\n",
    "    一个不依赖d2l的、功能完备的训练函数，支持多GPU。\n",
    "    这是 d2l.train_ch13 的一个独立实现。\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    # 将模型分发到指定的设备上\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices)\n",
    "    \n",
    "    print(f\"在 {devices} 上开始训练...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # 训练过程\n",
    "        net.train() # 设置为训练模式\n",
    "        train_loss_sum, train_acc_sum, num_samples = 0.0, 0.0, 0\n",
    "        \n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            # 将数据移动到主设备\n",
    "            X, y = X.to(devices), y.to(devices)\n",
    "            \n",
    "            trainer.zero_grad()\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.sum().backward()\n",
    "            trainer.step()\n",
    "            \n",
    "            # 累加指标\n",
    "            train_loss_sum += l.sum()\n",
    "            train_acc_sum += accuracy(y_hat, y)\n",
    "            num_samples += y.numel()\n",
    "\n",
    "        # 评估过程\n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "        \n",
    "        # 打印信息\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        train_loss_avg = train_loss_sum / num_samples\n",
    "        train_acc_avg = train_acc_sum / num_samples\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
    "              f'训练损失 {train_loss_avg:.4f}, '\n",
    "              f'训练精度 {train_acc_avg:.4f}, '\n",
    "              f'测试精度 {test_acc:.4f}, '\n",
    "              f'耗时 {epoch_time:.2f} 秒')\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n训练完成！总耗时 {total_time:.2f} 秒\")\n",
    "\n",
    "\n",
    "lr, num_epochs = 0.01, 5\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "devices = torch.device('mps' if torch.mps.is_available() else 'cpu')\n",
    "train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "    devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c46e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def predict_sentiment(net, vocab, sequence):\n",
    "    \"\"\"预测文本序列的情感\"\"\"\n",
    "    sequence = torch.tensor(vocab[sequence.split()], device=devices)\n",
    "    label = torch.argmax(net(sequence.reshape(1, -1)), dim=1)\n",
    "    return 'positive' if label == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d4f29f",
   "metadata": {},
   "source": [
    "# 情感分析：使用卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c7ab718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在本地找到已缓存的数据目录: ./data/aclImdb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "batch_size = 64\n",
    "train_iter, test_iter, vocab = load_data_imdb(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "888e5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr1d(X, K):\n",
    "    w = K.shape[0]\n",
    "    Y = torch.zeros((X.shape[0] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        Y[i] = (X[i: i + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b379406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.,  5.,  8., 11., 14., 17.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, K = torch.tensor([0, 1, 2, 3, 4, 5, 6]), torch.tensor([1, 2])\n",
    "corr1d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2631fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.,  8., 14., 20., 26., 32.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def corr1d_multi_in(X, K):\n",
    "    # 首先，遍历'X'和'K'的第0维（通道维）。然后，把它们加在一起\n",
    "    return sum(corr1d(x, k) for x, k in zip(X, K))\n",
    "\n",
    "X = torch.tensor([[0, 1, 2, 3, 4, 5, 6],\n",
    "              [1, 2, 3, 4, 5, 6, 7],\n",
    "              [2, 3, 4, 5, 6, 7, 8]])\n",
    "K = torch.tensor([[1, 2], [3, 4], [-1, -3]])\n",
    "corr1d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af29801e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
